#!/usr/bin/env python3
"""
Simple hardware detector that writes hardware_config.json at project root.

- Detects CPU thread count
- Tries to detect NVIDIA GPU VRAM via nvidia-smi (if available)
- Chooses reasonable defaults for llama.cpp parameters

This file is intentionally lightweight and safe to run on systems without NVIDIA GPUs.
"""

import json
import multiprocessing
import os
import subprocess
from typing import Optional, Dict, Any


def get_project_root() -> str:
    return os.path.dirname(os.path.abspath(__file__))


def get_cpu_threads() -> int:
    try:
        count = multiprocessing.cpu_count()
        return max(2, count)
    except Exception:
        return 4


def get_gpu_vram_mb() -> Optional[int]:
    """Return VRAM in MB for the first NVIDIA GPU, or None if not available."""
    try:
        # Query total VRAM (in MiB) without headers/units
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=memory.total",
                "--format=csv,noheader,nounits",
            ],
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            return None
        line = result.stdout.strip().splitlines()[0].strip()
        vram = int(line)
        # Some drivers report in MiB; treat as MB equivalent for our purposes
        return vram
    except Exception:
        return None


def get_gpu_name() -> Optional[str]:
    """Return the name of the first NVIDIA GPU, or None if not available."""
    try:
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-gpu=name",
                "--format=csv,noheader",
            ],
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            return None
        name = result.stdout.strip().splitlines()[0].strip()
        return name or None
    except Exception:
        return None


def compute_config(vram_mb: Optional[int], cpu_threads: int) -> Dict[str, Any]:
    """Compute llama.cpp-friendly settings from available hardware."""
    # Sensible defaults
    n_ctx = 4096
    n_threads = min(8, max(2, cpu_threads))
    gpu_layers = 0  # CPU-only by default

    if vram_mb is not None:
        # Heuristic mapping of VRAM capacity to offloaded layers
        # These are conservative to avoid OOM on smaller cards
        if vram_mb >= 16384:  # 16 GB+
            gpu_layers = -1  # Offload all layers
            n_ctx = 4096
        elif vram_mb >= 12288:  # 12 GB
            gpu_layers = 40
            n_ctx = 4096
        elif vram_mb >= 8192:  # 8 GB
            gpu_layers = 28
            n_ctx = 3072
        elif vram_mb >= 6144:  # 6 GB
            gpu_layers = 20
            n_ctx = 3072
        elif vram_mb >= 4096:  # 4 GB
            gpu_layers = 12
            n_ctx = 2048
        else:
            gpu_layers = 6
            n_ctx = 2048

    return {
        "n_threads": n_threads,
        "n_ctx": n_ctx,
        "gpu_layers": gpu_layers,
        "detected_vram_mb": vram_mb or 0,
        "detected_cpu_threads": cpu_threads,
        "detected_gpu_name": get_gpu_name() or "",
        "notes": "Auto-generated by hardware_detector.py",
    }


def write_config(config: Dict[str, Any], path: str) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)


def main() -> None:
    project_root = get_project_root()
    cpu_threads = get_cpu_threads()
    vram_mb = get_gpu_vram_mb()
    config = compute_config(vram_mb, cpu_threads)

    out_path = os.path.join(project_root, "hardware_config.json")
    write_config(config, out_path)

    print("✅ Hardware configuration generated:")
    print(json.dumps(config, indent=2))
    print(f"➡️  Written to: {out_path}")


if __name__ == "__main__":
    main()


